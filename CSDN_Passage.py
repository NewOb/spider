# 爬取CSDN博文，使用ip代理池和用户代理池import urllib.request,urllib.errorimport re,random,time# 设置uaua = [    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36",    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0",    "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)",    "Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1",]def c_ua(uas):    ua = random.choice(uas)    header = ("User-Agent",ua)    print("正在使用ua："+ua)    return header# 设置ip代理ip = [    "112.85.171.186:9999",    "1.197.203.230:9999",    "123.101.110.213:49516",    "114.239.253.118:9999"]def Proxy_IP(ips):    ip = random.choice(ips)    headers = c_ua(ua)    print("正在使用ip："+ip)    proxy = urllib.request.HTTPHandler({"http":ip})    opener = urllib.request.build_opener(proxy,urllib.request.HTTPHandler)    opener.addheaders = [headers]    urllib.request.install_opener(opener)Proxy_IP(ip)csdn = "https://www.csdn.net"data = urllib.request.urlopen(csdn).read().decode("utf-8","ignore")pat='<h2>\s*?<a href="(.*?)"'for i in range(150):    print(i)    try:        url = re.compile(pat).findall(data)        # url=list(set(url))        print(url)        # print(len(result))        # print(data)    except urllib.error.URLError as e:        if hasattr(e,"code"):            print(e.code)        if hasattr(e,"reson"):            print(e.reason)        time.sleep(5)        print("已睡眠5秒")# 下载文章页面for i in url:    if i:        passage = urllib.request.urlopen(i).read().decode("gbk","ignore")        p=i.split("/")        # urllib.request.urlretrieve(i,"E:/测试下载文档/"+str(p[-1])+".html")        fh = open("E:/测试下载文档/"+str(p[-1])+".html",mode="w")        fh.write(passage)        fh.close()    else:        continue