import urllib.request, urllib.errorimport re, random, time# 设置uaua = [    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36",    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0",    "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)",    "Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1",]# 从用户代理池中选取一个ua备用def c_ua(uas):    ua = random.choice(uas)    header = ("User-Agent", ua)    print("正在使用ua：" + ua)    return header# 设置ip代理ip = [    "112.85.171.186:9999",    "1.197.203.230:9999",    "123.101.110.213:49516",    "114.239.253.118:9999"]# 从ip代理池中选取一个ip备用def Proxy_IP(ips):    ip = random.choice(ips)    print("正在使用ip：" + ip)    return ip# 获取网页def get_HTML(url):    headers = c_ua(ua)    IP = Proxy_IP(ip)    proxy = urllib.request.HTTPHandler({"http": IP})    opener = urllib.request.build_opener(proxy, urllib.request.HTTPHandler)    opener.addheaders = [headers]    urllib.request.install_opener(opener)    csdn = url    try:        data = urllib.request.urlopen(csdn).read().decode("utf-8", "ignore")        return data    except Exception as e:        print("出现错误："+str(e))        time.sleep(5)        print("已睡眠5秒")        get_HTML(csdn)# 获取网页并利用正则找到匹配内容def get_content(url,pats):    data = get_HTML(url)    pat = pats    try:        url = re.compile(pat,re.S).findall(data)        # url=list(set(url))        # print(url)        return url        # print(len(result))        # print(data)    except Exception as e:        print("出现错误："+str(e))        time.sleep(5)        print("已睡眠5秒")        get_content(data,pat)# 写入文件def write_file(file_path,data):    fh = open(file_path,mode="w")    fh.write(data)    fh.close()