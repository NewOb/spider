import urllib.requestimport re# 打开网站# 建立需要提取内容的相应正则表达式# 提取data = urllib.request.urlopen("https://movie.douban.com/chart").read().decode("utf-8")str1 = "<a href=\"[a-z]{2,6}://(\w.*?) class=\".*?\">"result = re.compile(str1).findall(data)print(result)# try:#     pass# except Exception as error:#     pass# post请求import urllib.request,urllib.parseurl = "https://www.baidu.com"  #post目标站# 构造提交表单内容postdata = urllib.parse.urlencode({    "key1":"value1",    "key2":"value2",}).encode("utf-8")posturl = urllib.request.Request(url,postdata)urllib.request.urlopen(posturl).read().decode("utf-8")# 异常处理 HTTPError、URLError HTTPError是URLError的子集import urllib.requestimport urllib.errortry:    pass    # urllib.request.urlopen("http://www.baidu.com")except urllib.error.URLError as e:    # 判断状态吗，异常原因    if hasattr(e,"code"):        print(e.code)    if hasattr(e,"reson"):        print(e.reason)# 浏览器伪装# 修改报头使用urllib.request.build_opener()或者urllib.request.Request().add_header()实现# 包头格式 header=("User-Agent","value")headers=("User-Agent","Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36")opener = urllib.request.build_opener()opener.addheaders=[headers]urllib.request.install_opener(opener)      #安装为全局使urlopen带有headdata = opener.open(url).read()#用户代理池import randomuapools = [    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36",    "",    "",]def ua(uapools):    ua = random.choice(uapools)    headers=("User-Agent",ua)    opener = urllib.request.build_opener()    opener.addheaders = [headers]    urllib.request.install_opener(opener)#用户代理(池)# 和用户代理池类似，建立列表，写入IPip = "182.138.242.190"proxy = urllib.request.ProxyHandler({"http":ip})opener = urllib.request.build_opener(proxy,urllib.request.HTTPHandler)urllib.request.install_opener(opener)url = "www.baidu.com"urllib.request.urlopen(url).read().decode("utf-8","ignore")# ip代理池，接口方式，根据代理网站提供的构造url，每次先访问url获取IP